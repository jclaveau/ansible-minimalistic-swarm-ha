# - name: Check if OVH plugin exists
  # command: dpkg-query -l python3-certbot-dns-ovh
  # register: plugin_check
  # failed_when: false
    
# - include_tasks: certbot_admin-domain_debian.yml
  # when: plugin_check.stderr.find('no packages found') == -1

- name: Add hosts from inventory to /etc/hosts
  tags:
    - hosts
  blockinfile:
    path: /etc/hosts
    block: |
      {% for host in groups['all'] %}
      {% if hostvars[host]['ansible_host'] == ansible_default_ipv4.address %}
      127.0.0.1 {{ host }}
      {% else %}
      {{ hostvars[host]['ansible_host'] }} {{ host }}
      {% endif %}
      {% endfor %}

- name: dbg
  debug:
    var: volumes

- name: Ensure Gluster brick directories exist.
  file: "path={{ item.brick_dir }} state=directory mode=0775"
  loop: "{{ volumes }}"

- name: Ensure Gluster mount directories exist.
  file: "path={{ item.mount_dir }} state=directory mode=0775"
  loop: "{{ volumes }}"

# If reinstall
# stop gluster
# compare gluster uuid to the connected peers in the cluster
# flush /var/lib/glusterd, keeping only glusterd.info with the right id
# start gluster service
# remount volumes

# https://docs.ansible.com/ansible/latest/collections/gluster/gluster/gluster_volume_module.html
- name: Configure Gluster volume.
  debugger: on_failed
  gluster.gluster.gluster_volume:
    state: "{{ item.state | default('present') }}"
    name: "{{ item.volume_name }}"
    brick: "{{ item.brick_dir }}"
    replicas: "{{ item.replicas }}"
    cluster: "{{ ansible_play_hosts_all | join(',') }}"
    host: "{{ inventory_hostname }}"
    # quota: "{{ inventory_hostname }}"
    force: yes # allow the volume to be on the / partition 
    # TODO : configure a dedicated partition
  loop: "{{ volumes }}"
  run_once: true

# mount.glusterfs localhost:/swarm-shared-volume /mnt/gluster   
- name: Ensure Gluster volume is mounted.
  mount:
    name: "{{ item.mount_dir }}"
    src: "{{ inventory_hostname }}:/{{ item.volume_name }}"
    fstype: glusterfs
    opts: "defaults,_netdev"
    state: mounted
  when: item.state | default('present') == "present"
  loop: "{{ volumes }}"
    
# ls: cannot access '/mnt/gluster': Transport endpoint is not connected
- name: Ensure Gluster volume is available.
  shell: "ls {{ item.mount_dir }}"
  loop: "{{ volumes }}"
  register: ls_result
  failed_when: "'Transport endpoint is not connected' in ls_result.stderr"

- name: Get the current timestamp
  shell: "echo $(date +%T.%N)"
  register: timestamp
  run_once: true # possible workaround in case of parallelism issue: when: ansible_hostname == ansible_play_hosts[0]

- name: Create a test file on each volume
  shell: "echo {{ ansible_play_hosts[0] + ' ' + timestamp.stdout }} > {{ item.mount_dir }}/file_creation_test"
  run_once: true
  loop: "{{ volumes }}"

- name: Pause for 1 second to replicate the content on all nodes
  pause:
    seconds: 1

- name: Extract the timestamp from every volume on every node
  slurp:
    src: "{{ item.mount_dir }}/file_creation_test"
  register: read_timestamps
  loop: "{{ volumes }}"

- name: Check that every node has the same timestamp file
  assert:
    that:
      - 'item.content | b64decode == ansible_play_hosts[0] + " " + timestamp.stdout + "\n"'
    quiet: true
  with_items: "{{ read_timestamps.results }}"


